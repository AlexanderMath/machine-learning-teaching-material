{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%qtconsole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectation Maximization (theory)\n",
    "Please see slides 14-15 in `ML-Clustering-L2.pdf` for an explanation of the Expecation Maximization algorithm. Let $D = \\{x_1,...,x_n\\}\\subseteq \\mathbb{R}^d$. Recall that in the EM algorithm we represent a cluster $C_i$ by a Gaussian distribution whose density function is given by\n",
    "\n",
    "$$\n",
    "\\Pr(x|C_i) = \\frac{1}{\\sqrt{(2\\pi)^d|\\Sigma_i|}}\\cdot e^{-\\frac{1}{2}\\cdot (x-\\mu_i)^\\intercal\\cdot (\\Sigma_i)^{-1}\\cdot (x-\\mu_i)}.\n",
    "$$\n",
    "\n",
    "Each cluster $C_i$ is parameterized with a cluster mean $\\mu_i\\in\\Bbb R^d$, a covariance matrix $\\Sigma_i\\in\\Bbb R^{d\\times d}$ and a prior probability $\\Pr(C_i)$.\n",
    "\n",
    "<!--Given a clustering $M = \\{C_1,\\ldots,C_k\\}$, we can compute\n",
    "$$\n",
    "\\Pr(x) = \\sum_{i=1}^k \\Pr(C_i)\\cdot\\Pr(x|C_i),\n",
    "$$\n",
    "where we estimate $\\Pr(C_i)$ by $W_i$, the frequency of the cluster $C_i$ in the data (i.e. the ratio of data points belonging to cluster $C_i$).-->\n",
    "\n",
    "The Expectation Maximization has two steps: Expectation and Maximization (hence the name). Before this step we need to initialize an initial clustering $M = \\{C_1,\\ldots,C_k\\}$. This is done as follows: \n",
    "- Choose cluster mean $\\mu_i\\in\\Bbb R^d$ uniformly at random (in the adequate region). \n",
    "- Initialize the covariance matrix $\\Sigma_i\\in\\Bbb R^{d\\times d}$ as the identity matrix. \n",
    "- Initialize the prior probability $\\Pr(C_i)$ is to $\\frac{1}{k}$. \n",
    "\n",
    "\n",
    "Then we repeat the following two steps until the sum $\\sum_{i=1}^k \\|\\mu_i - \\mu'_i\\|$ is below some pre-specified prespecified threshold $\\epsilon$ (where $\\mu'_i$ and $\\mu_i$ are the means computed in two consecutive executions).\n",
    "\n",
    "- <b>Update probabilities (expectation step):</b> For each all pairs $C_i$ and $x_j$ compute $\\Pr(C_i|x_j) = \\frac{\\Pr(x_j|C_i)\\cdot \\Pr(C_i)}{\\Pr(x_j)}$. <br>\n",
    "  To do this we need to compute $\\Pr(x_j|C_i)$ and $\\Pr(x_j)$. Compute $\\Pr(x_j|C_i)$ by the formula above, and compute $\\Pr(x)$ by $\\Pr(x)= \\sum_{i=1}^k \\Pr(C_i)\\cdot\\Pr(x|C_i)$.<br><br>\n",
    "  \n",
    "- <b>Update clustering (maximization step): </b>Compute a new model $M = \\{C_1,\\ldots, C_k\\}$ by recomputing $W_i$, $\\mu_i$ and $\\Sigma_i$ as\n",
    "\n",
    "$$\n",
    "W_i = \\frac{1}{n}\\cdot\\sum_{x\\in D}\\Pr(C_i|x)\\approx P(C_i),\n",
    "\\qquad\n",
    "\\mu_i = \\frac{\\sum_{x\\in D}^n x\\Pr(C_i|x)}{\\sum_{x\\in D} \\Pr(C_i|x)},\n",
    "\\qquad\n",
    "\\Sigma_i = \\frac{\\sum_{x\\in D} \\Pr(C_i|x)\\cdot(x-\\mu_i)\\cdot(x-\\mu_i)^\\intercal}{\\sum_{x\\in D} \\Pr(C_i|x)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 1:</b> What is the objective of Expectation Maximization (EM) clustering problem? \n",
    "\n",
    "HINT: See slides pages 14-15. \n",
    "\n",
    "<!--<b>Question 1 (b): </b>Rewrite the objective for $\\text{dist}(x,y)=||x-y||$-->\n",
    "\n",
    "<b>Question 2:</b> What are the E-step, M-step? What is estimated, what is fixed?\n",
    "\n",
    "<b>Question 3:</b> Does EM algorithm provide any guarantees for finding the optimal solution?\n",
    "\n",
    "HINT: What is the optimal solution? \n",
    "\n",
    "<b>Question 4:</b> Can a cluster of a EM clustering be empty? \n",
    "\n",
    "HINT: Since we are dealing with probabilities, what would it mean for a cluster to be 'empty'?\n",
    "\n",
    "<b>Question 5:</b> In which of the two step does the EM algorithm attempt to maximize the objective function?\n",
    "\n",
    "<b>Question 6:</b> Is EM algorithm guaranteed to converge? \n",
    "\n",
    "HINT: Assume the objective is strictly decreasing. \n",
    "\n",
    "<b>Question 7 (a): </b>How is Expectation Maximization Clustering a generalizaton of k-means clustering?\n",
    "\n",
    "HINT: See the book [ZM] page 353. \n",
    "\n",
    "<b>Question 7 (b): </b>Could we assign a \"hard clustering\" of all points instead of a probability?\n",
    "\n",
    "HINT: Which cluster is it most likely that $x_i$ belongs to?\n",
    "\n",
    "<b>Question 7 (c): </b>Both models rely on certain assumptions on the data. What are these assumptions? Which assumptions are shared?\n",
    "\n",
    "<b>Question 7 (d): </b>Give examples of data where the assumptions of k-means clustering and EM clustering are reasonable/unreasonable. (relevant for the exam)\n",
    "\n",
    "<b>Bonus Question (Hard): </b>Baum-Welch training of Hidden Markov Models was also called Expectation Maximization. Are there any similarities/differences between the EM algorithm above and the Baum-Welch EM algorithm?\n",
    "\n",
    "<!--\n",
    "TODO: Some question that I thought about which would be nice to have is like a discussion about the differences between Kmeans and EM. The book points out (page 353) that EM is some kind of generalization of Kmeans, so in some sense it's more versatil (for example, it doesn't restrict us to convex clusters -right?- and it seems like the fact that the clusters are ''randomized'' allows us to do more things). What are the advantages/disadvantages of each? -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectation Maximization (code)\n",
    "In this exercise you must implement the EM algorithm. To test our implementation we will need data. Like last week we use the Iris dataset. Recall that the dataset has three clases so we <i>cheat</i> by setting $k=3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the Iris data set\n",
    "import sklearn.datasets\n",
    "iris = sklearn.datasets.load_iris()\n",
    "X = iris['data'][:,0:2] # reduce dimensions so we can plot what happens.\n",
    "k = 3\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing EM algorithm\n",
    "Remember the Expectation Maximization algorithm has two steps. Let us first implement the Expectation step. For this step it suffices to compute $\\Pr(C_i\\mid x_j)$ for $i=1,\\ldots,k$ and $j=1,\\ldots n$. Remember that $\\Pr(C_i \\mid x_j)$ can be computed as \n",
    "\n",
    "$$\\Pr(C_i|x_j) = \\frac{\\Pr(x_j|C_i)\\cdot \\Pr(C_i)}{\\Pr(x_j)}$$\n",
    "\n",
    "The first goal will then be to compute each part of the above equation. To do this we will need the parameters: ($\\mu_i, \\Sigma_i, \\Pr(C_i)$). These will be represented in Python as follows: \n",
    "\n",
    "$$\\text{means}=\\begin{pmatrix}\n",
    "- & \\mu_1^T & - \\\\\n",
    "- & \\vdots & - \\\\\n",
    "- & \\mu_k^T & - \n",
    "\\end{pmatrix}\\in \\mathbb{R}^{k\\times d} \\quad\\quad\n",
    "\\text{probs_c}=\\begin{pmatrix}\n",
    "\\Pr(C_1)\\\\\n",
    "\\vdots \\\\\n",
    "\\Pr(C_k)\n",
    "\\end{pmatrix}\\in \\mathbb{R}^{k}\n",
    "$$\n",
    "\n",
    "Similarly we represent the $\\Sigma_i$'s as $\\text{covs}\\in\\mathbb{R}^{k\\times d \\times d}$ such that $\\text{covs[i]}=\\Sigma_i$. Finally we represent $\\Pr(x)$'s and $\\Pr(C_i \\mid x)$ as \n",
    "\n",
    "$$\\text{prob_x}=\\begin{pmatrix}\n",
    "\\Pr(x_1) \\\\\n",
    "\\vdots \\\\\n",
    "\\Pr(x_n) \n",
    "\\end{pmatrix}\\in \\mathbb{R}^{k\\times d} \\quad\\quad\n",
    "\\text{probs_cx}=\\begin{pmatrix}\n",
    "\\Pr(C_1\\mid x_1) & \\ldots & \\Pr(C_1 \\mid x_n)\\\\\n",
    "\\vdots \\\\\n",
    "\\Pr(C_k \\mid x_1) & \\ldots & \\Pr(C_k \\mid x_n)\n",
    "\\end{pmatrix}\\in \\mathbb{R}^{k}\n",
    "$$\n",
    "\n",
    "The function *compute_probs_cs* below takes `means`, `probs_c` and `covs` as input and returns `prob_cx` and `prob_x`. \n",
    "\n",
    "<b>Question: </b>What is the dimensions of `prob_cx` and how can we compute it given `means`, `probs_c` and `covs`?\n",
    "\n",
    "<!-- The algorithm returns a clustering $M = \\{C_1,\\ldots,C_k\\}$. This corresponds to a \n",
    " - sequence of means $(\\mu_1,\\ldots,\\mu_k)$ where each $\\mu_i\\in\\Bbb R^d$,\n",
    " - sequence of covariance matrices $(\\Sigma_1,\\ldots,\\Sigma_k)$ where each $\\Sigma_i\\in\\Bbb R^{d\\times d}$,\n",
    " - prior probabilities $(\\Pr(C_1),\\ldots,\\Pr(C_k))$. \n",
    "\n",
    "Given $x$ we can then compute the probability of $x$ conditioned on the $i$'th cluster $Pr(C\\mid x_i)$. If we want to assign $x$ to a specific cluster we compute $(\\Pr(x|C_1),\\ldots,\\Pr(x|C_k))$ and assign $x$ to the `arg max`.-->\n",
    "\n",
    "<!--The following helper function takes a description of a Gaussian Mixture ($\\mu_i$'s, $\\Sigma_i$'s and $\\Pr(C_i)$'s)) and returns the probability densities of each point. We represent the inputs as \n",
    "\n",
    "If you want more information you can take a look at scipy's [multivariate_normal](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.multivariate_normal.html) documentation.-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "def compute_probs_cx(points, means, covs, probs_c):\n",
    "    '''\n",
    "    Input\n",
    "      - points: (n times d) array containing the dataset\n",
    "      - means:  (k times d) array containing the k means\n",
    "      - covs:   (k times d times d) array such that cov[j,:,:] is the covariance matrix of the j-th Gaussian.\n",
    "      - priors: (k) array containing priors\n",
    "    Output\n",
    "      - probs:  (k times n) array such that the entry (i,j) represents Pr(C_i|x_j)\n",
    "    '''\n",
    "    # Convert to numpy arrays.\n",
    "    points, means, covs, probs_c = np.asarray(points), np.asarray(means), np.asarray(covs), np.asarray(probs_c)\n",
    "    \n",
    "    # Get sizes\n",
    "    n, d = points.shape\n",
    "    k = means.shape[0]\n",
    "    \n",
    "    # Compute probabilities\n",
    "    # This will be a (k, n) matrix where the (i,j)'th entry is Pr(C_i)*Pr(x_j|C_i).\n",
    "    probs_cx = np.zeros((k, n))\n",
    "    for i in range(k):\n",
    "        try:\n",
    "            probs_cx[i] = probs_c[i] * multivariate_normal.pdf(mean=means[i], cov=covs[i], x=points)\n",
    "        except Exception as e:\n",
    "            print(\"Cov matrix got singular: \", e)\n",
    "    \n",
    "    # The sum of the j'th column of this matrix is P(x_j); why?\n",
    "    probs_x = np.sum(probs_cx, axis=0, keepdims=True) \n",
    "    assert probs_x.shape == (1, n)\n",
    "    \n",
    "    # Divide the j'th column by P(x_j). The the (i,j)'th then \n",
    "    # becomes Pr(C_i)*Pr(x_j)|C_i)/Pr(x_j) = Pr(C_i|x_j)\n",
    "    probs_cx = probs_cx / probs_x\n",
    "    \n",
    "    return probs_cx, probs_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the basic structure for the EM algorithm. For the Expectation step it calls the above code. You need to finish the Maximization step \n",
    "\n",
    "<b>Question: </b>Where in the code (and how) do we initialize `means`, `covs` and `probs_c`?\n",
    "\n",
    "<b>Question: </b>Which three things do you need to compute in the Maximization step?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def em_algorithm(X, k, T, epsilon = 0.001, means=None):\n",
    "    \"\"\" Clusters the data X into k clusters using the Expectation Maximization algorithm. \n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : Data matrix of shape (n, d)\n",
    "        k : Number of clusters.\n",
    "        T : Maximum number of iterations\n",
    "        epsilon :  Stopping criteria for the EM algorithm. Stops if the means of\n",
    "                   two consequtive iterations are less than epsilon.\n",
    "        means : (k times d) array containing the k initial means (optional)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        means:     (k, d) array containing the k means\n",
    "        covs:      (k, d, d) array such that cov[j,:,:] is the covariance matrix of \n",
    "                   the Gaussian of the j-th cluster\n",
    "        probs_c:   (k, ) containing the probability Pr[C_i] for i=0,...,k. \n",
    "        llh:       The log-likelihood of the clustering (this is the objective we want to maximize)\n",
    "    \"\"\"\n",
    "    n, d = X.shape\n",
    "    \n",
    "    # Initialize and validate mean\n",
    "    if means is None: \n",
    "        means = np.random.rand(k, d)\n",
    "\n",
    "    # Initialize cov, prior\n",
    "    probs_x  = np.zeros(n) \n",
    "    probs_cx = np.zeros((k, n)) \n",
    "    probs_c  = np.zeros(k) + np.random.rand(k)\n",
    "    \n",
    "    covs = np.zeros((k, d, d))\n",
    "    for i in range(k): covs[i] = np.identity(d)\n",
    "    probs_c = np.ones(k) / k\n",
    "    \n",
    "    # Column names\n",
    "    print(\"Iterations\\tLLH\")\n",
    "    \n",
    "    close = False\n",
    "    old_means = np.zeros_like(means)\n",
    "    iterations = 0\n",
    "    while not(close) and iterations < T:\n",
    "        old_means[:] = means \n",
    "\n",
    "        # Expectation step\n",
    "        probs_cx, probs_x = compute_probs_cx(X, means, covs, probs_c)\n",
    "        assert probs_cx.shape == (k, n)\n",
    "        \n",
    "        # Maximization step\n",
    "        # YOUR CODE HERE\n",
    "        # END CODE\n",
    "        \n",
    "        # Compute per-sample average log likelihood (llh) of this iteration     \n",
    "        llh = 1/n*np.sum(np.log(probs_x))\n",
    "        print(iterations+1, \"\\t\\t\", llh)\n",
    "\n",
    "        # Stop condition\n",
    "        dist = np.sqrt(((means - old_means) ** 2).sum(axis=1))\n",
    "        close = np.all(dist < epsilon)\n",
    "        iterations += 1\n",
    "        \n",
    "    # Validate output\n",
    "    assert means.shape == (k, d)\n",
    "    assert covs.shape == (k, d, d)\n",
    "    assert probs_c.shape == (k,)\n",
    "    return means, covs, probs_c, llh\n",
    "\n",
    "# Load the Iris data set\n",
    "import sklearn.datasets\n",
    "iris = sklearn.datasets.load_iris()\n",
    "X = iris['data'][:,0:2] # reduce dimensions so we can plot what happens.\n",
    "k = 3\n",
    "\n",
    "means, covs, priors, llh = em_algorithm(X, 3, 100, 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random initialization usually causes the algorithm to get stuck at different local maximum. This causes different runs to get different scores. In practice this is usually handled by running the algorithm several times and picking the best run. \n",
    "\n",
    "The following code runs EM algorithm 50 times and plots the score of each run. Because the data set is fairly small, $n=150$, most of the runs will get the same score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8,3))    \n",
    "llhs = []\n",
    "\n",
    "for i in range(100):\n",
    "    _, _, _, llh = em_algorithm(X, 3, 100)\n",
    "    llhs.append(llh)\n",
    "    ax.plot(llhs, 'bx')\n",
    "    fig.canvas.draw() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check your implementation you should run <a href=\"http://scikit-learn.org/stable/modules/mixture.html\">sklearn</a>'s implementation of the EM algorithm. You might want to take a look at the documentation to get a better understanding of what the algorithm is actually doing.\n",
    "\n",
    "<!--By default the implementation repeats the algorithm $10$ times and picks the best result. A sanity check for your implementation of Lloyd's algorithm is to check that the scores are roughly the same. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture as EM\n",
    "expectation_maximization = EM(n_components=3, init_params='random', covariance_type='diag', verbose=2, verbose_interval =1).fit(X)\n",
    "\n",
    "print(expectation_maximization.score(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a visual understanding of the algorithm, the following code visualizes each step of the algorithm. Similarly to previous week, it prints the centroids and the corresponding cluster each point is assinged to. Furthermore, because the clusters are represented as gaussians, the gaussians are plotted. They are first plotted by themselves, they are plotted together (by summing). \n",
    "\n",
    "To run the visualization you need to copy and paste your implementation from above. Then the code should run with no modifications. Running the visualization might be a bit slow; try change the `detail` parameter if you have any problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def em_algorithm_visualize(X, k, T, epsilon = 0.001, means=None, detail=20):\n",
    "    \"\"\" Clusters the data X into k clusters using the Expectation Maximization algorithm. \n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : Data matrix of shape (n, d)\n",
    "        k : Number of clusters.\n",
    "        T : Maximum number of iterations\n",
    "        epsilon :  Stopping criteria for the EM algorithm. Stops if the means of\n",
    "                   two consequtive iterations are less than epsilon.\n",
    "        means : (k times d) array containing the k initial means (optional)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        means:     (k, d) array containing the k means\n",
    "        covs:      (k, d, d) array such that cov[j,:,:] is the covariance matrix of \n",
    "                   the Gaussian of the j-th cluster\n",
    "        probs_c:   (k, ) containing the probability Pr[C_i] for i=0,...,k. \n",
    "        llh:       The log-likelihood of the clustering (this is the objective we want to maximize)\n",
    "    \"\"\"\n",
    "    n, d = X.shape\n",
    "    \n",
    "    # Visualization stuff. \n",
    "    fig, (ax, _, _, _, _) = plt.subplots(5, 1, figsize=(10,16)) \n",
    "    ax.axis('off')\n",
    "    colors = [\"r\", \"g\", \"b\"]\n",
    "    ax3d = fig.add_subplot(2, 1, 2, projection='3d')\n",
    "    ax3d1 = fig.add_subplot(3, 1, 2, projection='3d')\n",
    "    ax3d2 = fig.add_subplot(4, 1, 2, projection='3d')\n",
    "    ax3d3 = fig.add_subplot(5, 1, 2, projection='3d')\n",
    "    ax3d.set_axis_off()\n",
    "    ax3d1.set_axis_off()\n",
    "    ax3d2.set_axis_off()\n",
    "    ax3d3.set_axis_off()\n",
    "    \n",
    "    # Initialize and validate mean\n",
    "    if means is None: means = np.random.rand(k, d)\n",
    "\n",
    "    # Initialize \n",
    "    probs_x  = np.zeros(n) \n",
    "    probs_cx = np.zeros((k, n)) \n",
    "    probs_c  = np.zeros(k) \n",
    "    covs = np.zeros((k, d, d))\n",
    "    for i in range(k): covs[i] = np.identity(d)\n",
    "    probs_c = np.ones(k) / k\n",
    "\n",
    "    # END CODE\n",
    "    \n",
    "    # Column names\n",
    "    print(\"Iterations\\tLLH\")\n",
    "    \n",
    "    close = False\n",
    "    old_means = np.zeros_like(means)\n",
    "    iterations = 0\n",
    "    while not(close) and iterations < T:\n",
    "        old_means[:] = means # This allows us to actually copy the array mean\n",
    "\n",
    "        # Expectation step\n",
    "        probs_cx, probs_x = compute_probs_cx(X, means, covs, probs_c)\n",
    "        assert probs_cx.shape == (k, n)\n",
    "        \n",
    "        # Maximization step\n",
    "        # YOUR CODE HERE\n",
    "        # END CODE\n",
    "        \n",
    "        # Finish condition\n",
    "        dist = np.sqrt(((means - old_means) ** 2).sum(axis=1))\n",
    "        close = np.all(dist < epsilon)\n",
    "        iterations += 1\n",
    "        \n",
    "        \n",
    "        # !----------- VISUALIZATION CODE -----------!\n",
    "        centroids = means\n",
    "        # probs_cx's (i,j) is Pr[C_i, x_j]\n",
    "        # assign each x_i to the cluster C_i that maximizes P(C_i | x_j)\n",
    "        clustering = np.argmax(probs_cx, axis=0)\n",
    "        assert clustering.shape == (n,), clustering.shape\n",
    "        \n",
    "        # Draw clusters\n",
    "        ax.cla()\n",
    "        for j in range(k):\n",
    "            centroid = centroids[j]\n",
    "            c = colors[j]\n",
    "            ax.scatter(centroid[0], centroid[1], s=123, c=c, marker='^')\n",
    "            data = X[clustering==j]\n",
    "            x = data[:,0]\n",
    "            y = data[:,1]\n",
    "            ax.scatter(x, y, s=3, c=c)\n",
    "            \n",
    "        # draw 3d gaussians. \n",
    "        #Create grid and multivariate normal\n",
    "        xs = np.linspace(4,7, 50)\n",
    "        ys = np.linspace(2,4.5, 50)\n",
    "        Xs, Ys = np.meshgrid(xs, ys)\n",
    "        pos = np.empty(Xs.shape + (2,))\n",
    "        pos[:, :, 0] = Xs; pos[:, :, 1] = Ys\n",
    "        prob_space = sum([multivariate_normal(means[j], covs[j]).pdf(pos) for j in range(k)])\n",
    "\n",
    "        #Make a 3D plot\n",
    "        ax3d.cla()\n",
    "        ax3d1.cla()\n",
    "        ax3d2.cla()\n",
    "        ax3d3.cla()\n",
    "        ax3d.plot_surface(Xs, Ys, prob_space, cmap='viridis', linewidth=0)\n",
    "        ax3d1.plot_surface(Xs, Ys, multivariate_normal(means[0], covs[0]).pdf(pos), cmap='viridis', linewidth=0)\n",
    "        ax3d2.plot_surface(Xs, Ys, multivariate_normal(means[1], covs[1]).pdf(pos), cmap='viridis', linewidth=0)\n",
    "        ax3d3.plot_surface(Xs, Ys, multivariate_normal(means[2], covs[2]).pdf(pos), cmap='viridis', linewidth=0)\n",
    "        \n",
    "        fig.canvas.draw()\n",
    "        \n",
    "    # Validate output\n",
    "    assert means.shape == (k, d)\n",
    "    assert covs.shape == (k, d, d)\n",
    "    assert probs_c.shape == (k,)\n",
    "    return means, covs, probs_c, llhs\n",
    "\n",
    "# Load the Iris data set\n",
    "import sklearn.datasets\n",
    "iris = sklearn.datasets.load_iris()\n",
    "X = iris['data'][:,0:2] # reduce dimensions so we can plot what happens.\n",
    "k = 3\n",
    "\n",
    "# the higher the detail the slower plotting\n",
    "detail = 20 # 50 looks very nice but your computer might not be able to handle it. \n",
    "means, covs, priors, llh = em_algorithm_visualize(X, 3, 40, 0.001, detail=detail)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing EM with Lloyd's algorithm \n",
    "\n",
    "So far we have been initializing the means for the Expectation Maximization algorithm at random. We could also make a mix of Lloyd's algorithm and EM algorithm by running Lloyd's algorithm first to obtain the initial means for the EM algorithm. \n",
    "\n",
    "Begin by copying and pasting the implementation of Lloyd's algorithm from the previous week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lloyds_algorithm(X, k, T):\n",
    "    \"\"\" Clusters the data of X into k clusters using T iterations of Lloyd's algorithm. \n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : Data matrix of shape (n, d)\n",
    "        k : Number of clusters.\n",
    "        T : Maximum number of iterations to run Lloyd's algorithm. \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        clustering: A vector of shape (n, ) where the i'th entry holds the cluster of X[i].\n",
    "        centroids:  The centroids/average points of each cluster. \n",
    "        cost:       The cost of the clustering \n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then use this algorithm to initialize the means for the EM algorithm. For this notice that `em_algorithm` accepts an optional input for the initial means.\n",
    "\n",
    "Also, notice that the sklearn's implementation of the EM algorithm can take this initialization into account. Can you look through the documentation and find out what lines should be changed when we used sklearn before? This would be very useful for testing and comparing your implementation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question:</b> Why can we use Lloyd's algorithm to initialize the EM algorithm? Does it (always) give a better final clustering? If so, why is that the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the clustering using silhuette coefficient\n",
    "In the lecture Ira talked about how one can compare different clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def silhouette(data, clustering): \n",
    "    n, d = data.shape\n",
    "    k = np.unique(clustering)[-1]+1\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    silh = None\n",
    "    # END CODE\n",
    "\n",
    "    return silh\n",
    "\n",
    "silhouette(X, clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcualte the Silhouette coefficient for the EM clustering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering digits\n",
    "In previous weeks we did supervised learning on images of digits. In this exercise we will perform clustering on digits. Remember clustering can be considered a type of unsupervised learning. The main difference to what we did before is that  will attempt to find patterns in the data without using the labels.  \n",
    "\n",
    "You can use the AUDigits if you want. The following code uses a data set of images called MNIST. They are almost identical. The only reason for using MNIST is that we can import it with just two lines of code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"data/\")\n",
    "\n",
    "X = mnist.train.images\n",
    "y = mnist.train.labels\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code runs the Expectation Maimization algorithm on 5000 images from the MNIST dataset. It then visualizes the found centroids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture as EM\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# One cluster for each digit\n",
    "k = 10\n",
    "\n",
    "# Run EM algorithm on 1000 images from the MNIST dataset. \n",
    "\n",
    "expectation_maximization = EM(n_components=k, max_iter=10, init_params='kmeans', covariance_type='diag', verbose=1, verbose_interval =1).fit(X)\n",
    "print(expectation_maximization.score(X[:1000]))\n",
    "\n",
    "means = expectation_maximization.means_\n",
    "covs = expectation_maximization.covariances_\n",
    "      \n",
    "print(means.shape)\n",
    "fig, ax = plt.subplots(1, k, figsize=(8, 1))\n",
    "\n",
    "for i in range(k):\n",
    "    ax[i].imshow(means[i].reshape(28, 28), cmap='gray')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 1: </b>Why do the centroids look like images of digits? \n",
    "    \n",
    "Because our clusters are represented as gaussians, it is possible to sample points from them. Think of this as rolling a dice, but instead of getting a number, you get a picture of a digit. The last line has a call to `sample`. The last parameter specifies which cluster to sample from. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "import numpy as np\n",
    "\n",
    "def sample(means, covs, num):\n",
    "    mean = means[num]\n",
    "    cov = covs[num]\n",
    "     \n",
    "    fig, ax = plt.subplots(1, 10, figsize=(8, 1))\n",
    "    \n",
    "    for i in range(10):\n",
    "        img = multivariate_normal.rvs(mean=mean, cov=cov) # draw random sample   \n",
    "        ax[i].imshow(img.reshape(28, 28), cmap='gray') # draw the random sample\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
